{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f97fdeb4-c9ca-4970-8776-2e50b9f7c626",
   "metadata": {},
   "source": [
    "### Proyecto: Analitica de textos\n",
    "#### Caso: Elegibilidad de pacientes para ensayos clinicos\n",
    "#### Felipe Bedoya"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daffe37-25c2-44a1-b69b-1f28cf9fa5c3",
   "metadata": {},
   "source": [
    "Para este proyecto, se escogio utilizar el modelo regresion logistica para determinar a los pacientes en si son elegibles o si no. Antes de poder entrenar el modelo se debe realizar un preprocesamiento exaustivo en donde se utilice el modelo de Bag of Words para la vectorizaci贸n del texto, y despues realizar la lematizaci贸n del mismo. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffb1df4-c57f-4e25-9d6e-b90dc70441c5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 0. Importaci贸n de librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a343f673-a152-4b56-8d32-8b00f02da6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 25)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "import numpy as np\n",
    "np.random.seed(3301)\n",
    "import pandas as pd\n",
    "\n",
    "# Preprocesamiento de datos\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#para hacer balanceo de los features\n",
    "from imblearn.over_sampling import SMOTE\n",
    "# Para realizar la separaciond el conjunto de aprendizaje en entrenamiento y test\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Para evaluar el modelo\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "# Para busqueda de hiperparametros\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Para la validaci贸n cruzada\n",
    "from sklearn.model_selection import KFold\n",
    "#Librerias para la visualizacion\n",
    "import matplotlib.pyplot as plt\n",
    "#Seaborn\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)  # or 199\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46509cd-719d-4136-b84e-a9246ff7bb6a",
   "metadata": {},
   "source": [
    "### 1. Preprocesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84adb828-77bd-46ac-b563-34e13c482f05",
   "metadata": {},
   "source": [
    "Primero se deben cargar los datos y realizar un perfilamiento de estos, una vez se sabe el estado y pureza de los datos se pueden empezar a limpiar siguiendo lo establecido en el diccionario de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8456ce-43ce-4b88-86ca-3902370523ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importe\n",
    "df_eleg = pd.read_csv('../Datos/ElegibilidadEstudiantes/clinical_trials_on_cancer_data_clasificacion.csv', sep=',', encoding='utf-8', dtype='unicode')\n",
    "df_eleg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c450003-0000-4b1c-a0e6-6441fbc8fc5e",
   "metadata": {},
   "source": [
    "Vemos si existen datos nulos en el dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401c3b3e-8b9b-4da5-847c-6bc0296401f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eleg.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0385fdc-5618-493e-9fbc-7b65b32ec6d2",
   "metadata": {},
   "source": [
    "Los datos no tienen nulos pero estan presentados de maneras distintas. Algunos comienzan con comillas, otros son espacios. Debemos lograr entradas similares. Adicionalmente la informacion importante del paciente esta ubicada despues del primer punto. Adicionalmente vemos que los labels son categoricos y debemos volvernos numericos. Aunque un label encoder haria esta tarea con facilidad, queremos conservar la categoria implicita que ya traen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e61ca93-d4f0-4b4e-a306-74d3e3118b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor(df):\n",
    "    df['study_and_condition'] = df['study_and_condition'].replace('\"\"', '')\n",
    "    df['study_and_condition'] = df['study_and_condition'].str.strip(' ')\n",
    "    df['study_and_condition'] = df['study_and_condition'].str.split('.').str[1]\n",
    "    df.loc[df['label'] == '__label__0', 'label'] = 0\n",
    "    df.loc[df['label'] == '__label__1', 'label'] = 1\n",
    "preprocessor(df_eleg)\n",
    "print(df_eleg.describe())\n",
    "df_eleg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353fc6cb-c64e-4f8a-a48c-63d79a4261d2",
   "metadata": {},
   "source": [
    "Creamos el pipeline para facilitar el uso del modelo en produccion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3a61c2-3bd8-43e8-83e4-6170e5a5ad0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = [('preproc', FunctionTransformer(preprocessor))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f34216-7e24-4869-a298-823b2046dbf3",
   "metadata": {},
   "source": [
    "Ahora vamos a preprocesar el texto partiendolo en tokens y lematizandolo. Despues se utilizar un modelo de bag of words y finalmente tf-idf para identificar las palabras importantes. Aprovechamos el corpus de nltk para quitar palabras conectoras que generen ruido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a33767-c0e1-4552-8d38-ef13868cb0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = PorterStemmer()\n",
    "stop = stopwords.words('english')\n",
    "def tokenizer_porter(sentence):\n",
    "    tokens = sentence.split()\n",
    "    stemmed_tokens = [porter.stem(token) for token in tokens if token not in stop]\n",
    "    return ' '.join(stemmed_tokens)\n",
    "\n",
    "def transformer_tokenizer(df):\n",
    "    df['study_and_condition'] = df['study_and_condition'].apply(tokenizer_porter)\n",
    "    \n",
    "pre += [('porter', transformer_tokenizer)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c602a02-f2ac-42c7-985f-4abc6bb48c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_tokenizer(df_eleg)\n",
    "df_eleg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd820351-680b-41d7-8cc4-30825e6080a8",
   "metadata": {},
   "source": [
    "Ahora utilizamos el modelo Bag of Worda para traducir el texto a un vector numerico que representa las palabras en el mismo. Como la frecuencia de las palabras no importantes tiende a ser elevado entonces utilizamos tfidf para corregirlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbab44c-d60f-4a07-a275-b0e8bb3e889e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(strip_accents=None, lowercase=False, preprocessor=None)\n",
    "pre += [('tfidf', tfidf)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
